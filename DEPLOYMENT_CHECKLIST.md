# üöÄ Production Deployment Checklist

## Pre-Deployment Setup

### Phase 1: Wav2Lip Installation ‚úÖ
- [x] Clone Wav2Lip repository
- [x] Create Python 3.10 virtual environment
- [x] Install PyTorch + CUDA
- [x] Install dependencies (librosa, opencv, etc.)
- [ ] Download wav2lip_gan.pth model (96MB) üîÑ **IN PROGRESS**
- [x] Download s3fd.pth model (90MB)
- [x] Copy luna_base.png and test audio
- [ ] Test generation (15-20s expected)

### Phase 2: GPU Optimization ‚è≥
- [ ] Open Windows Graphics Settings
- [ ] Add Wav2Lip Python to high performance list:
  - `%USERPROFILE%\Downloads\Wav2Lip\venv\Scripts\python.exe`
- [ ] Add Avatar Service Python to high performance list:
  - `%USERPROFILE%\Downloads\OneDevelopment-Agent-main\OneDevelopment-Agent-main\avatar_service\venv\Scripts\python.exe`
- [ ] Restart terminal
- [ ] Verify NVIDIA GPU is being used (Task Manager)

### Phase 3: Integration ‚è≥
- [ ] Copy `wav2lip_generator.py` to avatar_service
- [ ] Copy `avatar_server_wav2lip.py` to avatar_service
- [ ] Test wav2lip_generator standalone
- [ ] Start avatar server with Wav2Lip
- [ ] Test generation via API endpoint

### Phase 4: Production Deployment ‚è≥
- [ ] Verify ngrok tunnel is running
- [ ] Update AWS backend with ngrok URL
- [ ] Test end-to-end from frontend
- [ ] Monitor first few generations
- [ ] Verify performance (15-20s)

---

## Quick Test Procedures

### Test 1: Wav2Lip Standalone

```powershell
cd %USERPROFILE%\Downloads\Wav2Lip
.\venv\Scripts\activate
python quick_test.py
```

**Expected:**
- ‚úÖ Generation time: 15-20 seconds
- ‚úÖ Video file created
- ‚úÖ Video plays correctly
- ‚úÖ Lip-sync is accurate

**If Fails:**
- Check models are downloaded
- Check GPU is NVIDIA (not Intel)
- Check logs for errors

---

### Test 2: Integration Test

```powershell
cd %USERPROFILE%\Downloads\OneDevelopment-Agent-main\OneDevelopment-Agent-main\avatar_service

# Copy files
copy %USERPROFILE%\Downloads\Wav2Lip\wav2lip_generator.py .

# Test the generator
.\venv\Scripts\activate
python wav2lip_generator.py
```

**Expected:**
- ‚úÖ Generator initializes
- ‚úÖ Test video generated
- ‚úÖ 15-20 second generation time

---

### Test 3: Avatar Server Test

```powershell
cd avatar_service
.\venv\Scripts\activate

# Add FFmpeg to PATH
$env:Path += ";%LOCALAPPDATA%\Microsoft\WinGet\Packages\Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe\ffmpeg-8.0.1-full_build\bin"

# Start server
python avatar_server_wav2lip.py
```

**Test with curl:**
```powershell
curl -X POST http://localhost:8000/generate `
  -H "Content-Type: application/json" `
  -d '{\"text\":\"Hello, I am Luna! Welcome to One Development!\",\"voice_id\":\"default\"}'
```

**Expected:**
- ‚úÖ Audio generates in 2-3s
- ‚úÖ Video generates in 15-20s
- ‚úÖ Returns video URL
- ‚úÖ Video plays correctly

---

### Test 4: End-to-End Test

1. **Ensure all services running:**
   - [ ] Avatar server: `http://localhost:8000`
   - [ ] ngrok tunnel: (note URL)
   - [ ] AWS backend: `http://<YOUR_SERVER_IP>:8000`
   - [ ] AWS frontend: `http://<YOUR_SERVER_IP>:3000`

2. **Update AWS backend:**
   ```bash
   ssh ubuntu@<YOUR_SERVER_IP>
   export AVATAR_SERVICE_URL=https://YOUR_NGROK_URL.ngrok-free.app
   # Restart backend
   ```

3. **Test from frontend:**
   - Open: `http://<YOUR_SERVER_IP>:3000/`
   - Click Luna interface
   - Ask: "What is One Development?"
   - **Expected:**
     - Audio plays in 2-3 seconds
     - Video loads in 17-23 seconds
     - Smooth playback
     - Professional quality

---

## Production Startup Commands

### Terminal 1: Avatar Server

```powershell
cd %USERPROFILE%\Downloads\OneDevelopment-Agent-main\OneDevelopment-Agent-main\avatar_service

# Activate environment
.\venv\Scripts\activate

# Add FFmpeg
$env:Path += ";%LOCALAPPDATA%\Microsoft\WinGet\Packages\Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe\ffmpeg-8.0.1-full_build\bin"

# Set quality mode (optional)
$env:VIDEO_QUALITY = "high"  # or "fast"

# Start server
python avatar_server_wav2lip.py
```

### Terminal 2: ngrok Tunnel

```powershell
cd %USERPROFILE%\Downloads\ngrok-v3-stable-windows-amd64
.\ngrok http 8000
```

**Note the URL:** `https://XXXXX.ngrok-free.app`

---

## Performance Monitoring

### Real-Time GPU Monitoring

```powershell
# Terminal 3: GPU Monitor
nvidia-smi -l 1
```

**Watch for:**
- GPU Utilization: Should be 80-100% during generation
- GPU Memory: Should use 2-3 GB
- GPU Temperature: Should stay under 80¬∞C

### Check Generation Times

Look for these in avatar server logs:
```
üé§ Generating audio...
‚úì Audio generated: XXX.mp3
‚ö° Generating video with Wav2Lip (high)...
   Expected time: 15-20 seconds
‚úì Video generated successfully!
   Time: 17.3s
```

**Target Times:**
- Audio: 2-3 seconds ‚úÖ
- Video: 15-20 seconds ‚úÖ
- Total: 17-23 seconds ‚úÖ

---

## Troubleshooting Guide

### Issue: Video generation taking > 1 minute

**Diagnosis:**
```powershell
# Check which GPU is being used
# Open Task Manager ‚Üí Performance tab
# Look at GPU 0 (Intel) vs GPU 1 (NVIDIA)
```

**Fix:**
1. See `WINDOWS_GPU_SETUP.md`
2. Set Graphics Settings to High Performance
3. Restart terminal
4. Test again

---

### Issue: "Model not found" error

**Diagnosis:**
```powershell
cd %USERPROFILE%\Downloads\Wav2Lip\checkpoints
dir *.pth
```

**Fix:**
```powershell
# Download models if missing
Invoke-WebRequest -Uri "https://github.com/Rudrabha/Wav2Lip/releases/download/models/wav2lip_gan.pth" -OutFile "wav2lip_gan.pth"

Invoke-WebRequest -Uri "https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth" -OutFile "s3fd.pth"
```

---

### Issue: "CUDA out of memory"

**Fix:**
1. Close other GPU applications
2. Restart avatar server
3. Use "fast" quality mode instead of "high"
4. Check GPU memory: `nvidia-smi`

---

### Issue: Poor lip-sync quality

**Check:**
1. Audio quality (use edge-tts, not gTTS)
2. Source image quality (luna_base.png resolution)
3. Video quality setting ("high" vs "fast")

**Fix:**
- Ensure using edge-tts (already configured)
- Use "high" quality mode
- Consider upscaling luna_base.png

---

## Success Criteria

### ‚úÖ System is Production-Ready When:

**Performance:**
- [ ] Audio generation: < 5 seconds (target: 2-3s)
- [ ] Video generation: < 30 seconds (target: 15-20s)
- [ ] Total response time: < 35 seconds (target: 17-23s)
- [ ] Consistent performance across multiple requests

**Quality:**
- [ ] Audio: Clear, natural, professional
- [ ] Video: High resolution, smooth playback
- [ ] Lip-sync: Accurate and natural
- [ ] Overall: ‚≠ê‚≠ê‚≠ê‚≠ê or better

**Reliability:**
- [ ] No crashes or errors
- [ ] Handles multiple concurrent requests
- [ ] Graceful fallback to audio-only if video fails
- [ ] Automatic cleanup of old files

**User Experience:**
- [ ] Fast enough that users don't leave
- [ ] Professional quality
- [ ] Smooth playback
- [ ] No glitches or artifacts

---

## Post-Deployment Monitoring

### First Hour

Monitor closely:
- [ ] Check logs every 10 minutes
- [ ] Watch GPU usage
- [ ] Time each generation
- [ ] Test from frontend regularly

### First Day

- [ ] Monitor error rates
- [ ] Track average generation times
- [ ] Collect user feedback
- [ ] Optimize based on patterns

### First Week

- [ ] Identify most common queries
- [ ] Pre-generate popular responses
- [ ] Tune quality settings based on usage
- [ ] Plan optimizations

---

## Optimization Opportunities

### Quick Wins (Week 1)

1. **Cache FAQ Videos**
   ```python
   # Pre-generate top 10-20 questions
   # Serve instantly from cache
   # 80% of queries = instant response!
   ```

2. **Adjust Quality Tiers**
   ```python
   # Use "fast" mode for simple questions
   # Use "high" mode for important questions
   # Balance speed vs quality
   ```

3. **Pre-load Models**
   ```python
   # Load Wav2Lip at startup
   # Keep models in memory
   # Faster subsequent generations
   ```

### Future Enhancements (Month 1)

1. **Multi-GPU Support**
   - If traffic increases
   - Load balance across GPUs
   - Higher throughput

2. **Queue System**
   - Handle concurrent requests
   - Priority queue for VIP
   - Background processing

3. **Advanced Caching**
   - Semantic search for similar queries
   - Return similar cached videos
   - Reduce generation load

---

## Emergency Procedures

### If System Goes Down

1. **Check avatar server:**
   ```powershell
   Get-Process python | Where-Object {$_.Path -like "*avatar_service*"}
   ```

2. **Restart if needed:**
   ```powershell
   cd avatar_service
   .\venv\Scripts\activate
   python avatar_server_wav2lip.py
   ```

3. **Check ngrok:**
   ```powershell
   # Get current URL
   curl http://127.0.0.1:4040/api/tunnels | ConvertFrom-Json
   ```

4. **Update AWS backend if URL changed**

### Fallback Mode

If video generation fails completely:
- System automatically falls back to audio-only
- Users still get voice responses
- Fix video generation without downtime

---

## Final Pre-Launch Checklist

### Before Going Live:

- [ ] All tests passed
- [ ] GPU optimized (NVIDIA in use)
- [ ] Wav2Lip generating in 15-20s
- [ ] Avatar server running
- [ ] ngrok tunnel active
- [ ] AWS backend configured
- [ ] Frontend tested
- [ ] Monitoring in place
- [ ] Backup plan ready
- [ ] Documentation complete

### Launch Readiness:

- [ ] Team briefed
- [ ] Monitoring dashboards open
- [ ] Logs being watched
- [ ] Ready to debug quickly
- [ ] Celebration planned! üéâ

---

## üéä You're Ready to Deploy!

**Everything is prepared:**
- ‚úÖ Ultra-fast video generation (15-20s)
- ‚úÖ Professional quality
- ‚úÖ Comprehensive documentation
- ‚úÖ Production-ready code
- ‚úÖ Monitoring & troubleshooting guides

**Next:** Wait for models to download, then test and deploy!

**Timeline:** 30 minutes to production! üöÄ


